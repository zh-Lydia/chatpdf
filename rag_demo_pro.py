
import os
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
# ä»¥ä¸Šä¸¤è¡Œæ·»åŠ çš„Hugging Faceé•œåƒè®¾ç½®ï¼Œæ˜¯ä¸ºäº†è§£å†³æ²¡æœ‰ç§‘å­¦ä¸Šç½‘ç¯å¢ƒä¸‹è½½å‘é‡æ¨¡å‹çš„é—®é¢˜
import gradio as gr
from pdfminer.high_level import extract_text_to_fp
from sentence_transformers import SentenceTransformer
# å¯¼å…¥äº¤å‰ç¼–ç å™¨
from sentence_transformers import CrossEncoder
import faiss # ĞĞ¾Ğ²Ñ‹Ğ¹ Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚
import requests
import json
from io import StringIO
from langchain_text_splitters import RecursiveCharacterTextSplitter
import os
import socket
import webbrowser
import logging
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import time
from datetime import datetime
import hashlib
import re
from dotenv import load_dotenv
# å¯¼å…¥BM25ç®—æ³•åº“
from rank_bm25 import BM25Okapi
import numpy as np # Ğ£Ğ±ĞµĞ´Ğ¸Ğ¼ÑÑ, Ñ‡Ñ‚Ğ¾ numpy Ğ¸Ğ¼Ğ¿Ğ¾Ñ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½
import jieba
import threading
from functools import lru_cache

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()
SERPAPI_KEY = os.getenv("SERPAPI_KEY")  # Ğ’ .env Ñ„Ğ°Ğ¹Ğ»Ğµ ÑƒÑÑ‚Ğ°Ğ½Ğ¾Ğ²Ğ¸Ñ‚Ğµ SERPAPI_KEY
SEARCH_ENGINE = "google"  # ĞœĞ¾Ğ¶Ğ½Ğ¾ Ğ¸Ğ·Ğ¼ĞµĞ½Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ´Ñ€ÑƒĞ³ÑƒÑ Ğ¿Ğ¾Ğ¸ÑĞºĞ¾Ğ²ÑƒÑ ÑĞ¸ÑÑ‚ĞµĞ¼Ñƒ Ğ¿Ñ€Ğ¸ Ğ½ĞµĞ¾Ğ±Ñ…Ğ¾Ğ´Ğ¸Ğ¼Ğ¾ÑÑ‚Ğ¸
# ĞĞ¾Ğ²Ğ¾Ğµ: ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¼ĞµÑ‚Ğ¾Ğ´Ğ° Ğ¿ĞµÑ€ĞµÑ€Ğ°Ğ½Ğ¶Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ (ĞºÑ€Ğ¾ÑÑ-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€ Ğ¸Ğ»Ğ¸ LLM)
RERANK_METHOD = os.getenv("RERANK_METHOD", "cross_encoder")  # "cross_encoder" Ğ¸Ğ»Ğ¸ "llm"
# ĞĞ¾Ğ²Ğ¾Ğµ: ĞšĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ SiliconFlow API
SILICONFLOW_API_KEY = os.getenv("SILICONFLOW_API_KEY")
SILICONFLOW_API_URL = os.getenv("SILICONFLOW_API_URL", "https://api.siliconflow.cn/v1/chat/completions")

# Ğ’ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ñ‚Ğ°Ğ¹Ğ¼Ğ°ÑƒÑ‚Ğ°
import requests
requests.adapters.DEFAULT_RETRIES = 3  # Ğ£Ğ²ĞµĞ»Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ ĞºĞ¾Ğ»Ğ¸Ñ‡ĞµÑÑ‚Ğ²Ğ¾ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚Ğ¾Ğº

# Ğ’ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ñ… Ğ¾ĞºÑ€ÑƒĞ¶ĞµĞ½Ğ¸Ñ
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'  # ĞÑ‚ĞºĞ»ÑÑ‡Ğ°ĞµĞ¼ Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ oneDNN

# Ğ’ ÑĞ°Ğ¼Ğ¾Ğ¼ Ğ½Ğ°Ñ‡Ğ°Ğ»Ğµ Ñ„Ğ°Ğ¹Ğ»Ğ° Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¿Ñ€Ğ¾ĞºÑĞ¸
import os
os.environ['NO_PROXY'] = 'localhost,127.0.0.1'  # ĞĞ¾Ğ²Ğ°Ñ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ° Ğ¾Ğ±Ñ…Ğ¾Ğ´Ğ° Ğ¿Ñ€Ğ¾ĞºÑĞ¸

# Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºĞ¾Ğ¼Ğ¿Ğ¾Ğ½ĞµĞ½Ñ‚Ğ¾Ğ²
EMBED_MODEL = SentenceTransformer('all-MiniLM-L6-v2')
# ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ´Ğ»Ñ ÑĞ¼Ğ±ĞµĞ´Ğ´Ğ¸Ğ½Ğ³Ğ¾Ğ² Ñ‚Ğ°ĞºĞ¶Ğµ Ğ¼Ğ¾Ğ¶Ğ½Ğ¾ Ğ¿ĞµÑ€ĞµĞºĞ»ÑÑ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ¼Ğ¾Ğ´ĞµĞ»ÑŒ, Ğ¾Ğ¿Ñ‚Ğ¸Ğ¼Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ´Ğ»Ñ ĞºĞ¸Ñ‚Ğ°Ğ¹ÑĞºĞ¾Ğ³Ğ¾ ÑĞ·Ñ‹ĞºĞ°, Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€:
# EMBED_MODEL = SentenceTransformer('shibing624/text2vec-base-chinese')

# FAISSç›¸å…³çš„ Ğ³Ğ»Ğ¾Ğ±Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿ĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ñ‹Ğµ
faiss_index = None
faiss_contents_map = {}  # original_id -> content
faiss_metadatas_map = {} # original_id -> metadata
faiss_id_order_for_index = [] # Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ Ğ¿Ğ¾Ñ€ÑĞ´Ğ¾Ğº ID, ĞºĞ°Ğº Ğ¾Ğ½Ğ¸ Ğ±Ñ‹Ğ»Ğ¸ Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ĞµĞ½Ñ‹ Ğ² FAISS

# ĞĞ¾Ğ²Ğ¾Ğµ: Ğ˜Ğ½Ğ¸Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ ĞºÑ€Ğ¾ÑÑ-ÑĞ½ĞºĞ¾Ğ´ĞµÑ€Ğ° (Ğ¾Ñ‚Ğ»Ğ¾Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ°)
cross_encoder = None
cross_encoder_lock = threading.Lock()

def get_cross_encoder():
    """å»¶è¿ŸåŠ è½½äº¤å‰ç¼–ç å™¨æ¨¡å‹"""
    global cross_encoder
    if cross_encoder is None:
        with cross_encoder_lock:
            if cross_encoder is None:
                try:
                    # ä½¿ç”¨å¤šè¯­è¨€äº¤å‰ç¼–ç å™¨ï¼Œæ›´é€‚åˆä¸­æ–‡
                    cross_encoder = CrossEncoder('sentence-transformers/distiluse-base-multilingual-cased-v2')
                    logging.info("äº¤å‰ç¼–ç å™¨åŠ è½½æˆåŠŸ")
                except Exception as e:
                    logging.error(f"åŠ è½½äº¤å‰ç¼–ç å™¨å¤±è´¥: {str(e)}")
                    # è®¾ç½®ä¸ºNoneï¼Œä¸‹æ¬¡è°ƒç”¨ä¼šé‡è¯•
                    cross_encoder = None
    return cross_encoder

# æ–°å¢ï¼šBM25ç´¢å¼•ç®¡ç†
def recursive_retrieval(initial_query, max_iterations=3, enable_web_search=False, model_choice="ollama"):
    """
    å®ç°é€’å½’æ£€ç´¢ä¸è¿­ä»£æŸ¥è¯¢åŠŸèƒ½
    é€šè¿‡åˆ†æå½“å‰æŸ¥è¯¢ç»“æœï¼Œç¡®å®šæ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢
    
    Args:
        initial_query: åˆå§‹æŸ¥è¯¢
        max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
        model_choice: ä½¿ç”¨çš„æ¨¡å‹é€‰æ‹©("ollama"æˆ–"siliconflow")
        
    Returns:
        åŒ…å«æ‰€æœ‰æ£€ç´¢å†…å®¹çš„åˆ—è¡¨
    """
    query = initial_query
    all_contexts = []
    all_doc_ids = []  # ä½¿ç”¨åŸå§‹ID
    all_metadata = []
    
    global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
    
    for i in range(max_iterations):
        logging.info(f"é€’å½’æ£€ç´¢è¿­ä»£ {i+1}/{max_iterations}ï¼Œå½“å‰æŸ¥è¯¢: {query}")
        
        web_results_texts = [] # Store text from web results for context building
        if enable_web_search and check_serpapi_key():
            try:
                # update_web_results now needs to handle FAISS directly or be adapted
                # For now, let's assume it returns texts to be added to context
                web_search_raw_results = update_web_results(query) # This function needs to be adapted for FAISS
                for res in web_search_raw_results:
                    text = f"æ ‡é¢˜ï¼š{res.get('title', '')}\\næ‘˜è¦ï¼š{res.get('snippet', '')}"
                    web_results_texts.append(text)
                    # We would also need to add these to faiss_index, faiss_contents_map etc.
                    # and get their FAISS indices if we want them to be part of semantic search.
                    # This part is complex due to dynamic addition and potential ID clashes.
                    # For now, web results are added as pure text context, not searched semantically *again* within this loop's FAISS query.
            except Exception as e:
                logging.error(f"ç½‘ç»œæœç´¢é”™è¯¯: {str(e)}")
        
        query_embedding = EMBED_MODEL.encode([query])
        query_embedding_np = np.array(query_embedding).astype('float32')
        
        semantic_results_docs = []
        semantic_results_metadatas = []
        semantic_results_ids = []

        if faiss_index and faiss_index.ntotal > 0:
            try:
                D, I = faiss_index.search(query_embedding_np, k=10) # D: distances, I: indices
                # I contains the internal FAISS indices. We need to map them back to original IDs.
                for faiss_idx in I[0]: # I[0] because query_embedding_np was a batch of 1
                    if faiss_idx != -1 and faiss_idx < len(faiss_id_order_for_index):
                        original_id = faiss_id_order_for_index[faiss_idx]
                        semantic_results_docs.append(faiss_contents_map.get(original_id, ""))
                        semantic_results_metadatas.append(faiss_metadatas_map.get(original_id, {}))
                        semantic_results_ids.append(original_id)
            except Exception as e:
                logging.error(f"FAISS æ£€ç´¢é”™è¯¯: {str(e)}")
        
        bm25_results = BM25_MANAGER.search(query, top_k=10) # BM25_MANAGER.search returns list of dicts
        
        # Adapt hybrid_merge to work with current data structures
        # It expects semantic_results in a specific format if we pass it directly
        # For now, prepare a structure similar to old semantic_results for hybrid_merge
        prepared_semantic_results_for_hybrid = {
            "ids": [semantic_results_ids],
            "documents": [semantic_results_docs],
            "metadatas": [semantic_results_metadatas]
        }

        hybrid_results = hybrid_merge(prepared_semantic_results_for_hybrid, bm25_results, alpha=0.7)
        
        doc_ids_current_iter = []
        docs_current_iter = []
        metadata_list_current_iter = []
        
        if hybrid_results:
            for doc_id, result_data in hybrid_results[:10]: # doc_id here is the original_id
                doc_ids_current_iter.append(doc_id)
                docs_current_iter.append(result_data['content'])
                metadata_list_current_iter.append(result_data['metadata'])
        
        if docs_current_iter:
            try:
                reranked_results = rerank_results(query, docs_current_iter, doc_ids_current_iter, metadata_list_current_iter, top_k=5)
            except Exception as e:
                logging.error(f"é‡æ’åºé”™è¯¯: {str(e)}")
                reranked_results = [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0}) 
                                  for doc_id, doc, meta in zip(doc_ids_current_iter, docs_current_iter, metadata_list_current_iter)]
        else:
            reranked_results = []
        
        current_contexts_for_llm = web_results_texts[:] # Start with web results for LLM context
        for doc_id, result_data in reranked_results:
            doc = result_data['content']
            metadata = result_data['metadata']
            
            if doc_id not in all_doc_ids:  
                all_doc_ids.append(doc_id)
                all_contexts.append(doc)
                all_metadata.append(metadata)
            current_contexts_for_llm.append(doc) # Add reranked local docs for LLM context
        
        if i == max_iterations - 1:
            break
            
        if current_contexts_for_llm: # Use combined web and local context for deciding next query
            current_summary = "\\n".join(current_contexts_for_llm[:3]) if current_contexts_for_llm else "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            
            next_query_prompt = f"""åŸºäºåŸå§‹é—®é¢˜: {initial_query}
ä»¥åŠå·²æ£€ç´¢ä¿¡æ¯: 
{current_summary}

åˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢ã€‚å¦‚æœéœ€è¦ï¼Œè¯·æä¾›æ–°çš„æŸ¥è¯¢é—®é¢˜ï¼Œä½¿ç”¨ä¸åŒè§’åº¦æˆ–æ›´å…·ä½“çš„å…³é”®è¯ã€‚
å¦‚æœå·²ç»æœ‰å……åˆ†ä¿¡æ¯ï¼Œè¯·å›å¤'ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢'ã€‚

æ–°æŸ¥è¯¢(å¦‚æœéœ€è¦):"""
            
            try:
                if model_choice == "siliconflow":
                    logging.info("ä½¿ç”¨SiliconFlow APIåˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢")
                    next_query_result = call_siliconflow_api(next_query_prompt, temperature=0.7, max_tokens=256)
                    # SiliconFlow APIè¿”å›æ ¼å¼åŒ…å«å›ç­”å’Œå¯èƒ½çš„æ€ç»´é“¾ï¼Œè¿™é‡Œåªéœ€è¦å›ç­”éƒ¨åˆ†æ¥åˆ¤æ–­æ˜¯å¦ç»§ç»­
                    # å‡è®¾call_siliconflow_apiè¿”å›çš„æ˜¯ä¸€ä¸ªå…ƒç»„ (å›ç­”, æ€ç»´é“¾) æˆ–åªæ˜¯å›ç­”å­—ç¬¦ä¸²
                    if isinstance(next_query_result, tuple):
                         next_query = next_query_result[0].strip() # å–å›ç­”éƒ¨åˆ†
                    else:
                         next_query = next_query_result.strip() # å¦‚æœåªè¿”å›å­—ç¬¦ä¸²

                    # ç§»é™¤æ½œåœ¨çš„æ€ç»´é“¾æ ‡è®°
                    if "<think>" in next_query:
                        next_query = next_query.split("<think>")[0].strip()


                else:
                    logging.info("ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹åˆ†ææ˜¯å¦éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢")
                    response = session.post(
                        "http://localhost:11434/api/generate",
                        json={
                            "model": "deepseek-r1:1.5b",
                            "prompt": next_query_prompt,
                            "stream": False
                        },
                        timeout=30
                    )
                    # Ollama è¿”å›æ ¼å¼ä¸åŒï¼Œéœ€è¦æ ¹æ®å®é™…æƒ…å†µæå–
                    next_query = response.json().get("response", "").strip()
                
                if "ä¸éœ€è¦" in next_query or "ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢" in next_query or len(next_query) < 5:
                    logging.info("LLMåˆ¤æ–­ä¸éœ€è¦è¿›ä¸€æ­¥æŸ¥è¯¢ï¼Œç»“æŸé€’å½’æ£€ç´¢")
                    break
                    
                # ä½¿ç”¨æ–°æŸ¥è¯¢ç»§ç»­è¿­ä»£
                query = next_query
                logging.info(f"ç”Ÿæˆæ–°æŸ¥è¯¢: {query}")
            except Exception as e:
                logging.error(f"ç”Ÿæˆæ–°æŸ¥è¯¢æ—¶å‡ºé”™: {str(e)}")
                break
        else:
            # å¦‚æœå½“å‰è¿­ä»£æ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ï¼Œç»“æŸè¿­ä»£
            break
    
    return all_contexts, all_doc_ids, all_metadata

class BM25IndexManager:
    def __init__(self):
        self.bm25_index = None
        self.doc_mapping = {}  # æ˜ å°„BM25ç´¢å¼•ä½ç½®åˆ°æ–‡æ¡£ID
        self.tokenized_corpus = []
        self.raw_corpus = []
        
    def build_index(self, documents, doc_ids):
        """æ„å»ºBM25ç´¢å¼•"""
        self.raw_corpus = documents
        self.doc_mapping = {i: doc_id for i, doc_id in enumerate(doc_ids)}
        
        # å¯¹æ–‡æ¡£è¿›è¡Œåˆ†è¯ï¼Œä½¿ç”¨jiebaåˆ†è¯å™¨æ›´é€‚åˆä¸­æ–‡
        self.tokenized_corpus = []
        for doc in documents:
            # å¯¹ä¸­æ–‡æ–‡æ¡£è¿›è¡Œåˆ†è¯
            tokens = list(jieba.cut(doc))
            self.tokenized_corpus.append(tokens)
        
        # åˆ›å»ºBM25ç´¢å¼•
        self.bm25_index = BM25Okapi(self.tokenized_corpus)
        return True
        
    def search(self, query, top_k=5):
        """ä½¿ç”¨BM25æ£€ç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.bm25_index:
            return []
        
        # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
        tokenized_query = list(jieba.cut(query))
        
        # è·å–BM25å¾—åˆ†
        bm25_scores = self.bm25_index.get_scores(tokenized_query)
        
        # è·å–å¾—åˆ†æœ€é«˜çš„æ–‡æ¡£ç´¢å¼•
        top_indices = np.argsort(bm25_scores)[-top_k:][::-1]
        
        # è¿”å›ç»“æœ
        results = []
        for idx in top_indices:
            if bm25_scores[idx] > 0:  # åªè¿”å›æœ‰ç›¸å…³æ€§çš„ç»“æœ
                results.append({
                    'id': self.doc_mapping[idx],
                    'score': float(bm25_scores[idx]),
                    'content': self.raw_corpus[idx]
                })
        
        return results
    
    def clear(self):
        """æ¸…ç©ºç´¢å¼•"""
        self.bm25_index = None
        self.doc_mapping = {}
        self.tokenized_corpus = []
        self.raw_corpus = []

# åˆå§‹åŒ–BM25ç´¢å¼•ç®¡ç†å™¨
BM25_MANAGER = BM25IndexManager()

logging.basicConfig(level=logging.INFO)

print("Gradio version:", gr.__version__)  # æ·»åŠ ç‰ˆæœ¬è¾“å‡º

# åœ¨åˆå§‹åŒ–ç»„ä»¶åæ·»åŠ ï¼š
session = requests.Session()
retries = Retry(
    total=3,
    backoff_factor=0.1,
    status_forcelist=[500, 502, 503, 504]
)
session.mount('http://', HTTPAdapter(max_retries=retries))

#########################################
# SerpAPI ç½‘ç»œæŸ¥è¯¢åŠå‘é‡åŒ–å¤„ç†å‡½æ•°
#########################################
def serpapi_search(query: str, num_results: int = 5) -> list:
    """
    æ‰§è¡Œ SerpAPI æœç´¢ï¼Œå¹¶è¿”å›è§£æåçš„ç»“æ„åŒ–ç»“æœ
    """
    if not SERPAPI_KEY:
        raise ValueError("æœªè®¾ç½® SERPAPI_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
    try:
        params = {
            "engine": SEARCH_ENGINE,
            "q": query,
            "api_key": SERPAPI_KEY,
            "num": num_results,
            "hl": "zh-CN",  # ä¸­æ–‡ç•Œé¢
            "gl": "cn"
        }
        response = requests.get("https://serpapi.com/search", params=params, timeout=15)
        response.raise_for_status()
        search_data = response.json()
        return _parse_serpapi_results(search_data)
    except Exception as e:
        logging.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        return []

def _parse_serpapi_results(data: dict) -> list:
    """è§£æ SerpAPI è¿”å›çš„åŸå§‹æ•°æ®"""
    results = []
    if "organic_results" in data:
        for item in data["organic_results"]:
            result = {
                "title": item.get("title"),
                "url": item.get("link"),
                "snippet": item.get("snippet"),
                "timestamp": item.get("date")  # è‹¥æœ‰æ—¶é—´ä¿¡æ¯ï¼Œå¯é€‰
            }
            results.append(result)
    # å¦‚æœæœ‰çŸ¥è¯†å›¾è°±ä¿¡æ¯ï¼Œä¹Ÿå¯ä»¥æ·»åŠ ç½®é¡¶ï¼ˆå¯é€‰ï¼‰
    if "knowledge_graph" in data:
        kg = data["knowledge_graph"]
        results.insert(0, {
            "title": kg.get("title"),
            "url": kg.get("source", {}).get("link", ""),
            "snippet": kg.get("description"),
            "source": "knowledge_graph"
        })
    return results

def update_web_results(query: str, num_results: int = 5) -> list:
    """
    åŸºäº SerpAPI æœç´¢ç»“æœã€‚æ³¨æ„ï¼šæ­¤ç‰ˆæœ¬ä¸å°†ç»“æœå­˜å…¥FAISSã€‚
    å®ƒä»…è¿”å›åŸå§‹æœç´¢ç»“æœã€‚
    """
    results = serpapi_search(query, num_results)
    if not results:
        logging.info("ç½‘ç»œæœç´¢æ²¡æœ‰è¿”å›ç»“æœæˆ–å‘ç”Ÿé”™è¯¯")
        return []
    
    # ä¹‹å‰è¿™é‡Œæœ‰åˆ é™¤æ—§ç½‘ç»œç»“æœå’Œæ·»åŠ åˆ°ChromaDBçš„é€»è¾‘ã€‚
    # ç”±äºFAISS IndexFlatL2ä¸æ”¯æŒæŒ‰IDåˆ é™¤ï¼Œå¹¶ä¸”åŠ¨æ€æ·»åŠ æ¶‰åŠå¤æ‚IDç®¡ç†ï¼Œ
    # æ­¤ç®€åŒ–ç‰ˆæœ¬ä¸å°†ç½‘ç»œç»“æœæ·»åŠ åˆ°FAISSç´¢å¼•ã€‚
    # è¿”å›åŸå§‹ç»“æœï¼Œä¾›è°ƒç”¨è€…å†³å®šå¦‚ä½•ä½¿ç”¨ï¼ˆä¾‹å¦‚ï¼Œä»…ä½œä¸ºæ–‡æœ¬ä¸Šä¸‹æ–‡ï¼‰ã€‚
    logging.info(f"ç½‘ç»œæœç´¢è¿”å› {len(results)} æ¡ç»“æœï¼Œè¿™äº›ç»“æœä¸ä¼šè¢«æ·»åŠ åˆ°FAISSç´¢å¼•ä¸­ã€‚")
    return results # è¿”å›åŸå§‹SerpAPIç»“æœåˆ—è¡¨

# æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY
def check_serpapi_key():
    """æ£€æŸ¥æ˜¯å¦é…ç½®äº†SERPAPI_KEY"""
    return SERPAPI_KEY is not None and SERPAPI_KEY.strip() != ""

# æ·»åŠ æ–‡ä»¶å¤„ç†çŠ¶æ€è·Ÿè¸ª
class FileProcessor:
    def __init__(self):
        self.processed_files = {}  # å­˜å‚¨å·²å¤„ç†æ–‡ä»¶çš„çŠ¶æ€
        
    def clear_files(self):
        """æ¸…ç©ºæ‰€æœ‰æ–‡ä»¶è®°å½•"""
        self.processed_files = {}
        
    def add_file(self, file_name):
        self.processed_files[file_name] = {
            'status': 'ç­‰å¾…å¤„ç†',
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'chunks': 0
        }
        
    def update_status(self, file_name, status, chunks=None):
        if file_name in self.processed_files:
            self.processed_files[file_name]['status'] = status
            if chunks is not None:
                self.processed_files[file_name]['chunks'] = chunks
                
    def get_file_list(self):
        return [
            f"ğŸ“„ {fname} | {info['status']}"
            for fname, info in self.processed_files.items()
        ]

file_processor = FileProcessor()

#########################################
# çŸ›ç›¾æ£€æµ‹å‡½æ•°
#########################################
def detect_conflicts(sources):
    """ç²¾å‡†çŸ›ç›¾æ£€æµ‹ç®—æ³•"""
    key_facts = {}
    for item in sources:
        facts = extract_facts(item['text'] if 'text' in item else item.get('excerpt', ''))
        for fact, value in facts.items():
            if fact in key_facts:
                if key_facts[fact] != value:
                    return True
            else:
                key_facts[fact] = value
    return False

def extract_facts(text):
    """ä»æ–‡æœ¬æå–å…³é”®äº‹å®ï¼ˆç¤ºä¾‹é€»è¾‘ï¼‰"""
    facts = {}
    # æå–æ•°å€¼å‹äº‹å®
    numbers = re.findall(r'\b\d{4}å¹´|\b\d+%', text)
    if numbers:
        facts['å…³é”®æ•°å€¼'] = numbers
    # æå–æŠ€æœ¯æœ¯è¯­
    if "äº§ä¸šå›¾è°±" in text:
        facts['æŠ€æœ¯æ–¹æ³•'] = list(set(re.findall(r'[A-Za-z]+æ¨¡å‹|[A-Z]{2,}ç®—æ³•', text)))
    return facts

def evaluate_source_credibility(source):
    """è¯„ä¼°æ¥æºå¯ä¿¡åº¦"""
    credibility_scores = {
        "gov.cn": 0.9,
        "edu.cn": 0.85,
        "weixin": 0.7,
        "zhihu": 0.6,
        "baidu": 0.5
    }
    
    url = source.get('url', '')
    if not url:
        return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦
    
    domain_match = re.search(r'//([^/]+)', url)
    if not domain_match:
        return 0.5
    
    domain = domain_match.group(1)
    
    # æ£€æŸ¥æ˜¯å¦åŒ¹é…ä»»ä½•å·²çŸ¥åŸŸå
    for known_domain, score in credibility_scores.items():
        if known_domain in domain:
            return score
    
    return 0.5  # é»˜è®¤ä¸­ç­‰å¯ä¿¡åº¦

def extract_text(filepath):
    """æ”¹è¿›çš„PDFæ–‡æœ¬æå–æ–¹æ³•"""
    output = StringIO()
    with open(filepath, 'rb') as file:
        extract_text_to_fp(file, output)
    return output.getvalue()

def process_multiple_pdfs(files, progress=gr.Progress()):
    """å¤„ç†å¤šä¸ªPDFæ–‡ä»¶"""
    if not files:
        return "è¯·é€‰æ‹©è¦ä¸Šä¼ çš„PDFæ–‡ä»¶", []
    
    try:
        # æ¸…ç©ºå‘é‡æ•°æ®åº“å’Œç›¸å…³å­˜å‚¨
        progress(0.1, desc="æ¸…ç†å†å²æ•°æ®...")
        global faiss_index, faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
        faiss_index = None
        faiss_contents_map = {}
        faiss_metadatas_map = {}
        faiss_id_order_for_index = []
        
        # æ¸…ç©ºBM25ç´¢å¼•
        BM25_MANAGER.clear()
        logging.info("æˆåŠŸæ¸…ç†å†å²FAISSæ•°æ®å’ŒBM25ç´¢å¼•")
        
        # æ¸…ç©ºæ–‡ä»¶å¤„ç†çŠ¶æ€
        file_processor.clear_files()
        
        total_files = len(files)
        processed_results = []
        total_chunks = 0
        
        all_new_chunks = []
        all_new_metadatas = []
        all_new_original_ids = []
        
        for idx, file in enumerate(files, 1):
            try:
                file_name = os.path.basename(file.name)
                progress((idx-1)/total_files, desc=f"å¤„ç†æ–‡ä»¶ {idx}/{total_files}: {file_name}")
                
                file_processor.add_file(file_name)
                text = extract_text(file.name)
                
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=400,        
                    chunk_overlap=40,     
                    separators=["\n\n", "\n", "ã€‚", "ï¼Œ", "ï¼›", "ï¼š", " ", ""]
                )
                chunks = text_splitter.split_text(text)
                
                if not chunks:
                    raise ValueError("æ–‡æ¡£å†…å®¹ä¸ºç©ºæˆ–æ— æ³•æå–æ–‡æœ¬")
                
                doc_id = f"doc_{int(time.time())}_{idx}"
                
                # Store chunks and metadatas temporarily before batch embedding
                current_file_ids = [f"{doc_id}_chunk_{i}" for i in range(len(chunks))]
                current_file_metadatas = [{"source": file_name, "doc_id": doc_id} for _ in chunks]

                all_new_chunks.extend(chunks)
                all_new_metadatas.extend(current_file_metadatas)
                all_new_original_ids.extend(current_file_ids)
                
                total_chunks += len(chunks)
                file_processor.update_status(file_name, "å¤„ç†å®Œæˆ", len(chunks))
                processed_results.append(f"âœ… {file_name}: æˆåŠŸå¤„ç† {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
            except Exception as e:
                error_msg = str(e)
                logging.error(f"å¤„ç†æ–‡ä»¶ {file_name} æ—¶å‡ºé”™: {error_msg}")
                file_processor.update_status(file_name, f"å¤„ç†å¤±è´¥: {error_msg}")
                processed_results.append(f"âŒ {file_name}: å¤„ç†å¤±è´¥ - {error_msg}")
        
        if all_new_chunks:
            progress(0.8, desc="ç”Ÿæˆæ–‡æœ¬åµŒå…¥...")
            embeddings = EMBED_MODEL.encode(all_new_chunks, show_progress_bar=True)
            embeddings_np = np.array(embeddings).astype('float32')
            
            progress(0.9, desc="æ„å»ºFAISSç´¢å¼•...")
            if faiss_index is None: # Should always be None here due to clearing
                dimension = embeddings_np.shape[1]
                faiss_index = faiss.IndexFlatL2(dimension)
            
            faiss_index.add(embeddings_np)
            
            for i, original_id in enumerate(all_new_original_ids):
                faiss_contents_map[original_id] = all_new_chunks[i]
                faiss_metadatas_map[original_id] = all_new_metadatas[i]
            faiss_id_order_for_index.extend(all_new_original_ids) # Keep track of order for FAISS indices
            logging.info(f"FAISSç´¢å¼•æ„å»ºå®Œæˆï¼Œå…±ç´¢å¼• {faiss_index.ntotal} ä¸ªæ–‡æœ¬å—")

        summary = f"\næ€»è®¡å¤„ç† {total_files} ä¸ªæ–‡ä»¶ï¼Œ{total_chunks} ä¸ªæ–‡æœ¬å—"
        processed_results.append(summary)
        
        progress(0.95, desc="æ„å»ºBM25æ£€ç´¢ç´¢å¼•...")
        update_bm25_index() # This will need to use faiss_contents_map
        
        file_list = file_processor.get_file_list()
        
        return "\n".join(processed_results), file_list
        
    except Exception as e:
        error_msg = str(e)
        logging.error(f"æ•´ä½“å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}")
        return f"å¤„ç†è¿‡ç¨‹å‡ºé”™: {error_msg}", []

# æ–°å¢ï¼šäº¤å‰ç¼–ç å™¨é‡æ’åºå‡½æ•°
def rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k=5):
    """
    ä½¿ç”¨äº¤å‰ç¼–ç å™¨å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        top_k: è¿”å›ç»“æœæ•°é‡
        
    è¿”å›:
        é‡æ’åºåçš„ç»“æœåˆ—è¡¨ [(doc_id, {'content': doc, 'metadata': metadata, 'score': score}), ...]
    """
    if not docs:
        return []
        
    encoder = get_cross_encoder()
    if encoder is None:
        logging.warning("äº¤å‰ç¼–ç å™¨ä¸å¯ç”¨ï¼Œè·³è¿‡é‡æ’åº")
        # è¿”å›åŸå§‹é¡ºåºï¼ˆæŒ‰ç´¢å¼•æ’åºï¼‰
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]
    
    # å‡†å¤‡äº¤å‰ç¼–ç å™¨è¾“å…¥
    cross_inputs = [[query, doc] for doc in docs]
    
    try:
        # è®¡ç®—ç›¸å…³æ€§å¾—åˆ†
        scores = encoder.predict(cross_inputs)
        
        # ç»„åˆç»“æœ
        results = [
            (doc_id, {
                'content': doc, 
                'metadata': meta,
                'score': float(score)  # ç¡®ä¿æ˜¯PythonåŸç”Ÿç±»å‹
            }) 
            for doc_id, doc, meta, score in zip(doc_ids, docs, metadata_list, scores)
        ]
        
        # æŒ‰å¾—åˆ†æ’åº
        results = sorted(results, key=lambda x: x[1]['score'], reverse=True)
        
        # è¿”å›å‰Kä¸ªç»“æœ
        return results[:top_k]
    except Exception as e:
        logging.error(f"äº¤å‰ç¼–ç å™¨é‡æ’åºå¤±è´¥: {str(e)}")
        # å‡ºé”™æ—¶è¿”å›åŸå§‹é¡ºåº
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]

# æ–°å¢ï¼šLLMç›¸å…³æ€§è¯„åˆ†å‡½æ•°
@lru_cache(maxsize=32)
def get_llm_relevance_score(query, doc):
    """
    ä½¿ç”¨LLMå¯¹æŸ¥è¯¢å’Œæ–‡æ¡£çš„ç›¸å…³æ€§è¿›è¡Œè¯„åˆ†ï¼ˆå¸¦ç¼“å­˜ï¼‰
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        doc: æ–‡æ¡£å†…å®¹
        
    è¿”å›:
        ç›¸å…³æ€§å¾—åˆ† (0-10)
    """
    try:
        # æ„å»ºè¯„åˆ†æç¤ºè¯
        prompt = f"""ç»™å®šä»¥ä¸‹æŸ¥è¯¢å’Œæ–‡æ¡£ç‰‡æ®µï¼Œè¯„ä¼°å®ƒä»¬çš„ç›¸å…³æ€§ã€‚
        è¯„åˆ†æ ‡å‡†ï¼š0åˆ†è¡¨ç¤ºå®Œå…¨ä¸ç›¸å…³ï¼Œ10åˆ†è¡¨ç¤ºé«˜åº¦ç›¸å…³ã€‚
        åªéœ€è¿”å›ä¸€ä¸ª0-10ä¹‹é—´çš„æ•´æ•°åˆ†æ•°ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–è§£é‡Šã€‚
        
        æŸ¥è¯¢: {query}
        
        æ–‡æ¡£ç‰‡æ®µ: {doc}
        
        ç›¸å…³æ€§åˆ†æ•°(0-10):"""
        
        # è°ƒç”¨æœ¬åœ°LLM
        response = session.post(
            "http://localhost:11434/api/generate",
            json={
                "model": "deepseek-r1:1.5b",  # ä½¿ç”¨è¾ƒå°æ¨¡å‹è¿›è¡Œè¯„åˆ†
                "prompt": prompt,
                "stream": False
            },
            timeout=30
        )
        
        # æå–å¾—åˆ†
        result = response.json().get("response", "").strip()
        
        # å°è¯•è§£æä¸ºæ•°å­—
        try:
            score = float(result)
            # ç¡®ä¿åˆ†æ•°åœ¨0-10èŒƒå›´å†…
            score = max(0, min(10, score))
            return score
        except ValueError:
            # å¦‚æœæ— æ³•è§£æä¸ºæ•°å­—ï¼Œå°è¯•ä»æ–‡æœ¬ä¸­æå–æ•°å­—
            match = re.search(r'\b([0-9]|10)\b', result)
            if match:
                return float(match.group(1))
            else:
                # é»˜è®¤è¿”å›ä¸­ç­‰ç›¸å…³æ€§
                return 5.0
                
    except Exception as e:
        logging.error(f"LLMè¯„åˆ†å¤±è´¥: {str(e)}")
        # é»˜è®¤è¿”å›ä¸­ç­‰ç›¸å…³æ€§
        return 5.0

def rerank_with_llm(query, docs, doc_ids, metadata_list, top_k=5):
    """
    ä½¿ç”¨LLMå¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        top_k: è¿”å›ç»“æœæ•°é‡
    
    è¿”å›:
        é‡æ’åºåçš„ç»“æœåˆ—è¡¨
    """
    if not docs:
        return []
    
    results = []
    
    # å¯¹æ¯ä¸ªæ–‡æ¡£è¿›è¡Œè¯„åˆ†
    for doc_id, doc, meta in zip(doc_ids, docs, metadata_list):
        # è·å–LLMè¯„åˆ†
        score = get_llm_relevance_score(query, doc)
        
        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        results.append((doc_id, {
            'content': doc, 
            'metadata': meta,
            'score': score / 10.0  # å½’ä¸€åŒ–åˆ°0-1
        }))
    
    # æŒ‰å¾—åˆ†æ’åº
    results = sorted(results, key=lambda x: x[1]['score'], reverse=True)
    
    # è¿”å›å‰Kä¸ªç»“æœ
    return results[:top_k]

# æ–°å¢ï¼šé€šç”¨é‡æ’åºå‡½æ•°
def rerank_results(query, docs, doc_ids, metadata_list, method=None, top_k=5):
    """
    å¯¹æ£€ç´¢ç»“æœè¿›è¡Œé‡æ’åº
    
    å‚æ•°:
        query: æŸ¥è¯¢å­—ç¬¦ä¸²
        docs: æ–‡æ¡£å†…å®¹åˆ—è¡¨
        doc_ids: æ–‡æ¡£IDåˆ—è¡¨
        metadata_list: å…ƒæ•°æ®åˆ—è¡¨
        method: é‡æ’åºæ–¹æ³• ("cross_encoder", "llm" æˆ– None)
        top_k: è¿”å›ç»“æœæ•°é‡
        
    è¿”å›:
        é‡æ’åºåçš„ç»“æœ
    """
    # å¦‚æœæœªæŒ‡å®šæ–¹æ³•ï¼Œä½¿ç”¨å…¨å±€é…ç½®
    if method is None:
        method = RERANK_METHOD
    
    # æ ¹æ®æ–¹æ³•é€‰æ‹©é‡æ’åºå‡½æ•°
    if method == "llm":
        return rerank_with_llm(query, docs, doc_ids, metadata_list, top_k)
    elif method == "cross_encoder":
        return rerank_with_cross_encoder(query, docs, doc_ids, metadata_list, top_k)
    else:
        # é»˜è®¤ä¸è¿›è¡Œé‡æ’åºï¼ŒæŒ‰åŸå§‹é¡ºåºè¿”å›
        return [(doc_id, {'content': doc, 'metadata': meta, 'score': 1.0 - idx/len(docs)}) 
                for idx, (doc_id, doc, meta) in enumerate(zip(doc_ids, docs, metadata_list))]

def stream_answer(question, enable_web_search=False, model_choice="ollama", progress=gr.Progress()):
    """æ”¹è¿›çš„æµå¼é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼Œä»¥åŠå¤šç§æ¨¡å‹é€‰æ‹©"""
    global faiss_index # ç¡®ä¿å¯ä»¥è®¿é—®
    try:
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0
        if not knowledge_base_exists:
                if not enable_web_search:
                    yield "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚", "é‡åˆ°é”™è¯¯"
                    return
                else:
                    logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")
        
        progress(0.3, desc="æ‰§è¡Œé€’å½’æ£€ç´¢...")
        # ä½¿ç”¨é€’å½’æ£€ç´¢è·å–æ›´å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡
        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(
            initial_query=question,
            max_iterations=3,
            enable_web_search=enable_web_search,
            model_choice=model_choice
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []
        
        # ä½¿ç”¨æ£€ç´¢åˆ°çš„ç»“æœæ„å»ºä¸Šä¸‹æ–‡
        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')
            
            source_item = {
                'text': doc,
                'type': source_type
            }
            
            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source
            
            sources_for_conflict_detection.append(source_item)
        
        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)
        
        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection 
                               if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]
        
        context = "\n\n".join(context_with_sources)
        
        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])
        
        # æ”¹è¿›æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜å›ç­”è´¨é‡
        prompt_template = """ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œä½ éœ€è¦åŸºäºä»¥ä¸‹{context_type}å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æä¾›çš„å‚è€ƒå†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·éµå¾ªä»¥ä¸‹å›ç­”åŸåˆ™ï¼š
1. ä»…åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†
2. å¦‚æœå‚è€ƒå†…å®¹ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ä½ æ— æ³•å›ç­”
3. å›ç­”åº”è¯¥å…¨é¢ã€å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ®µè½å’Œç»“æ„
4. è¯·ç”¨ä¸­æ–‡å›ç­”
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æº{time_instruction}{conflict_instruction}

è¯·ç°åœ¨å¼€å§‹å›ç­”ï¼š"""
        
        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search and knowledge_base_exists else ("ç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£"),
            context=context if context else ("ç½‘ç»œæœç´¢ç»“æœå°†ç”¨äºå›ç­”ã€‚" if enable_web_search and not knowledge_base_exists else "çŸ¥è¯†åº“ä¸ºç©ºæˆ–æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"),
            question=question,
            time_instruction="ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„ä¿¡æ¯" if time_sensitive and enable_web_search else "",
            conflict_instruction="ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚" if conflict_detected else ""
        )
        
        progress(0.7, desc="ç”Ÿæˆå›ç­”...")
        full_answer = ""
        
        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
        if model_choice == "siliconflow":
            # å¯¹äºSiliconFlow APIï¼Œä¸æ”¯æŒæµå¼å“åº”ï¼Œæ‰€ä»¥ä¸€æ¬¡æ€§è·å–
            progress(0.8, desc="é€šè¿‡SiliconFlow APIç”Ÿæˆå›ç­”...")
            full_answer = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)
            
            # å¤„ç†æ€ç»´é“¾
            if "<think>" in full_answer and "</think>" in full_answer:
                processed_answer = process_thinking_content(full_answer)
            else:
                processed_answer = full_answer
                
            yield processed_answer, "å®Œæˆ!"
        else:
            # ä½¿ç”¨æœ¬åœ°Ollamaæ¨¡å‹çš„æµå¼å“åº”
            response = session.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "deepseek-r1:1.5b",
                    "prompt": prompt,
                    "stream": True
                },
                timeout=120,
                stream=True
            )
            
            for line in response.iter_lines():
                if line:
                    chunk = json.loads(line.decode()).get("response", "")
                    full_answer += chunk
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å®Œæ•´çš„æ€ç»´é“¾æ ‡ç­¾å¯ä»¥å¤„ç†
                    if "<think>" in full_answer and "</think>" in full_answer:
                        # éœ€è¦ç¡®ä¿å®Œæ•´æ”¶é›†ä¸€ä¸ªæ€ç»´é“¾ç‰‡æ®µåå†æ˜¾ç¤º
                        processed_answer = process_thinking_content(full_answer)
                    else:
                        processed_answer = full_answer
                    
                    yield processed_answer, "ç”Ÿæˆå›ç­”ä¸­..."
                    
            # å¤„ç†æœ€ç»ˆè¾“å‡ºï¼Œç¡®ä¿åº”ç”¨æ€ç»´é“¾å¤„ç†
            final_answer = process_thinking_content(full_answer)
            yield final_answer, "å®Œæˆ!"
        
    except Exception as e:
        yield f"ç³»ç»Ÿé”™è¯¯: {str(e)}", "é‡åˆ°é”™è¯¯"

def query_answer(question, enable_web_search=False, model_choice="ollama", progress=gr.Progress()):
    """é—®ç­”å¤„ç†æµç¨‹ï¼Œæ”¯æŒè”ç½‘æœç´¢ã€æ··åˆæ£€ç´¢å’Œé‡æ’åºï¼Œä»¥åŠå¤šç§æ¨¡å‹é€‰æ‹©"""
    global faiss_index # ç¡®ä¿å¯ä»¥è®¿é—®
    try:
        logging.info(f"æ”¶åˆ°é—®é¢˜ï¼š{question}ï¼Œè”ç½‘çŠ¶æ€ï¼š{enable_web_search}ï¼Œæ¨¡å‹é€‰æ‹©ï¼š{model_choice}")
        
        # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦ä¸ºç©º
        knowledge_base_exists = faiss_index is not None and faiss_index.ntotal > 0
        if not knowledge_base_exists:
                if not enable_web_search:
                    return "âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œè¯·å…ˆä¸Šä¼ æ–‡æ¡£ã€‚"
                else:
                    logging.warning("çŸ¥è¯†åº“ä¸ºç©ºï¼Œå°†ä»…ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœ")
        
        progress(0.3, desc="æ‰§è¡Œé€’å½’æ£€ç´¢...")
        # ä½¿ç”¨é€’å½’æ£€ç´¢è·å–æ›´å…¨é¢çš„ç­”æ¡ˆä¸Šä¸‹æ–‡
        all_contexts, all_doc_ids, all_metadata = recursive_retrieval(
            initial_query=question,
            max_iterations=3,
            enable_web_search=enable_web_search,
            model_choice=model_choice
        )
        
        # ç»„åˆä¸Šä¸‹æ–‡ï¼ŒåŒ…å«æ¥æºä¿¡æ¯
        context_with_sources = []
        sources_for_conflict_detection = []
        
        # ä½¿ç”¨æ£€ç´¢åˆ°çš„ç»“æœæ„å»ºä¸Šä¸‹æ–‡
        for doc, doc_id, metadata in zip(all_contexts, all_doc_ids, all_metadata):
            source_type = metadata.get('source', 'æœ¬åœ°æ–‡æ¡£')
            
            source_item = {
                'text': doc,
                'type': source_type
            }
            
            if source_type == 'web':
                url = metadata.get('url', 'æœªçŸ¥URL')
                title = metadata.get('title', 'æœªçŸ¥æ ‡é¢˜')
                context_with_sources.append(f"[ç½‘ç»œæ¥æº: {title}] (URL: {url})\n{doc}")
                source_item['url'] = url
                source_item['title'] = title
            else:
                source = metadata.get('source', 'æœªçŸ¥æ¥æº')
                context_with_sources.append(f"[æœ¬åœ°æ–‡æ¡£: {source}]\n{doc}")
                source_item['source'] = source
            
            sources_for_conflict_detection.append(source_item)
        
        # æ£€æµ‹çŸ›ç›¾
        conflict_detected = detect_conflicts(sources_for_conflict_detection)
        
        # è·å–å¯ä¿¡æº
        if conflict_detected:
            credible_sources = [s for s in sources_for_conflict_detection 
                              if s['type'] == 'web' and evaluate_source_credibility(s) > 0.7]
        
        context = "\n\n".join(context_with_sources)
        
        # æ·»åŠ æ—¶é—´æ•æ„Ÿæ£€æµ‹
        time_sensitive = any(word in question for word in ["æœ€æ–°", "ä»Šå¹´", "å½“å‰", "æœ€è¿‘", "åˆšåˆš"])
        
        # æ”¹è¿›æç¤ºè¯æ¨¡æ¿ï¼Œæé«˜å›ç­”è´¨é‡
        prompt_template = """ä½œä¸ºä¸€ä¸ªä¸“ä¸šçš„é—®ç­”åŠ©æ‰‹ï¼Œä½ éœ€è¦åŸºäºä»¥ä¸‹{context_type}å›ç­”ç”¨æˆ·é—®é¢˜ã€‚

æä¾›çš„å‚è€ƒå†…å®¹ï¼š
{context}

ç”¨æˆ·é—®é¢˜ï¼š{question}

è¯·éµå¾ªä»¥ä¸‹å›ç­”åŸåˆ™ï¼š
1. ä»…åŸºäºæä¾›çš„å‚è€ƒå†…å®¹å›ç­”é—®é¢˜ï¼Œä¸è¦ä½¿ç”¨ä½ è‡ªå·±çš„çŸ¥è¯†
2. å¦‚æœå‚è€ƒå†…å®¹ä¸­æ²¡æœ‰è¶³å¤Ÿä¿¡æ¯ï¼Œè¯·å¦è¯šå‘ŠçŸ¥ä½ æ— æ³•å›ç­”
3. å›ç­”åº”è¯¥å…¨é¢ã€å‡†ç¡®ã€æœ‰æ¡ç†ï¼Œå¹¶ä½¿ç”¨é€‚å½“çš„æ®µè½å’Œç»“æ„
4. è¯·ç”¨ä¸­æ–‡å›ç­”
5. åœ¨å›ç­”æœ«å°¾æ ‡æ³¨ä¿¡æ¯æ¥æº{time_instruction}{conflict_instruction}

è¯·ç°åœ¨å¼€å§‹å›ç­”ï¼š"""
        
        prompt = prompt_template.format(
            context_type="æœ¬åœ°æ–‡æ¡£å’Œç½‘ç»œæœç´¢ç»“æœ" if enable_web_search and knowledge_base_exists else ("ç½‘ç»œæœç´¢ç»“æœ" if enable_web_search else "æœ¬åœ°æ–‡æ¡£"),
            context=context if context else ("ç½‘ç»œæœç´¢ç»“æœå°†ç”¨äºå›ç­”ã€‚" if enable_web_search and not knowledge_base_exists else "çŸ¥è¯†åº“ä¸ºç©ºæˆ–æœªæ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚"),
            question=question,
            time_instruction="ï¼Œä¼˜å…ˆä½¿ç”¨æœ€æ–°çš„ä¿¡æ¯" if time_sensitive and enable_web_search else "",
            conflict_instruction="ï¼Œå¹¶æ˜ç¡®æŒ‡å‡ºä¸åŒæ¥æºçš„å·®å¼‚" if conflict_detected else ""
        )
        
        progress(0.8, desc="ç”Ÿæˆå›ç­”...")
        
        # æ ¹æ®æ¨¡å‹é€‰æ‹©ä½¿ç”¨ä¸åŒçš„API
        if model_choice == "siliconflow":
            # ä½¿ç”¨SiliconFlow API
            result = call_siliconflow_api(prompt, temperature=0.7, max_tokens=1536)
            
            # å¤„ç†æ€ç»´é“¾
            processed_result = process_thinking_content(result)
            return processed_result
        else:
            # ä½¿ç”¨æœ¬åœ°Ollama
            response = session.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": "deepseek-r1:7b",
                    "prompt": prompt,
                    "stream": False
                },
                timeout=120,  # å»¶é•¿åˆ°2åˆ†é’Ÿ
                headers={'Connection': 'close'}  # æ·»åŠ è¿æ¥å¤´
            )
            response.raise_for_status()  # æ£€æŸ¥HTTPçŠ¶æ€ç 
            
            progress(1.0, desc="å®Œæˆ!")
            # ç¡®ä¿è¿”å›å­—ç¬¦ä¸²å¹¶å¤„ç†ç©ºå€¼
            result = response.json()
            return process_thinking_content(str(result.get("response", "æœªè·å–åˆ°æœ‰æ•ˆå›ç­”")))
            
    except json.JSONDecodeError:
        return "å“åº”è§£æå¤±è´¥ï¼Œè¯·é‡è¯•"
    except KeyError:
        return "å“åº”æ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æœåŠ¡"
    except Exception as e:
        progress(1.0, desc="é‡åˆ°é”™è¯¯")  # ç¡®ä¿è¿›åº¦æ¡å®Œæˆ
        return f"ç³»ç»Ÿé”™è¯¯: {str(e)}"

def process_thinking_content(text):
    """å¤„ç†åŒ…å«<think>æ ‡ç­¾çš„å†…å®¹ï¼Œå°†å…¶è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
    # æ£€æŸ¥è¾“å…¥æ˜¯å¦ä¸ºæœ‰æ•ˆæ–‡æœ¬
    if text is None:
        return ""
    
    # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
    if not isinstance(text, str):
        try:
            processed_text = str(text)
        except:
            return "æ— æ³•å¤„ç†çš„å†…å®¹æ ¼å¼"
    else:
        processed_text = text
    
    # å¤„ç†æ€ç»´é“¾æ ‡ç­¾
    try:
        while "<think>" in processed_text and "</think>" in processed_text:
            start_idx = processed_text.find("<think>")
            end_idx = processed_text.find("</think>")
            if start_idx != -1 and end_idx != -1 and end_idx > start_idx:
                thinking_content = processed_text[start_idx + 7:end_idx]
                before_think = processed_text[:start_idx]
                after_think = processed_text[end_idx + 8:]
                
                # ä½¿ç”¨å¯æŠ˜å è¯¦æƒ…æ¡†æ˜¾ç¤ºæ€ç»´é“¾
                processed_text = before_think + "\n\n<details>\n<summary>æ€è€ƒè¿‡ç¨‹ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>\n\n" + thinking_content + "\n\n</details>\n\n" + after_think
        
        # å¤„ç†å…¶ä»–HTMLæ ‡ç­¾ï¼Œä½†ä¿ç•™detailså’Œsummaryæ ‡ç­¾
        processed_html = []
        i = 0
        while i < len(processed_text):
            if processed_text[i:i+8] == "<details" or processed_text[i:i+9] == "</details" or \
               processed_text[i:i+8] == "<summary" or processed_text[i:i+9] == "</summary":
                # ä¿ç•™è¿™äº›æ ‡ç­¾
                tag_end = processed_text.find(">", i)
                if tag_end != -1:
                    processed_html.append(processed_text[i:tag_end+1])
                    i = tag_end + 1
                    continue
            
            if processed_text[i] == "<":
                processed_html.append("&lt;")
            elif processed_text[i] == ">":
                processed_html.append("&gt;")
            else:
                processed_html.append(processed_text[i])
            i += 1
        
        processed_text = "".join(processed_html)
    except Exception as e:
        logging.error(f"å¤„ç†æ€ç»´é“¾å†…å®¹æ—¶å‡ºé”™: {str(e)}")
        # å‡ºé”™æ—¶è‡³å°‘è¿”å›åŸå§‹æ–‡æœ¬ï¼Œä½†ç¡®ä¿å®‰å…¨å¤„ç†HTMLæ ‡ç­¾
        try:
            return text.replace("<", "&lt;").replace(">", "&gt;")
        except:
            return "å¤„ç†å†…å®¹æ—¶å‡ºé”™"
    
    return processed_text

def call_siliconflow_api(prompt, temperature=0.7, max_tokens=1024):
    """
    è°ƒç”¨SiliconFlow APIè·å–å›ç­”
    
    Args:
        prompt: æç¤ºè¯
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        
    Returns:
        ç”Ÿæˆçš„å›ç­”æ–‡æœ¬å’Œæ€ç»´é“¾å†…å®¹
    """
    # æ£€æŸ¥æ˜¯å¦é…ç½®äº†SiliconFlow APIå¯†é’¥
    if not SILICONFLOW_API_KEY:
        logging.error("æœªè®¾ç½® SILICONFLOW_API_KEY ç¯å¢ƒå˜é‡ã€‚è¯·åœ¨.envæ–‡ä»¶ä¸­è®¾ç½®æ‚¨çš„ API å¯†é’¥ã€‚")
        return "é”™è¯¯ï¼šæœªé…ç½® SiliconFlow API å¯†é’¥ã€‚", ""

    try:
        payload = {
            "model": "Pro/deepseek-ai/DeepSeek-R1",
            "messages": [
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            "stream": False,
            "max_tokens": max_tokens,
            "stop": None,
            "temperature": temperature,
            "top_p": 0.7,
            "top_k": 50,
            "frequency_penalty": 0.5,
            "n": 1,
            "response_format": {"type": "text"}
        }

        headers = {
            "Authorization": f"Bearer {SILICONFLOW_API_KEY}", # ä»ç¯å¢ƒå˜é‡è·å–å¯†é’¥
            "Content-Type": "application/json; charset=utf-8" # æ˜ç¡®æŒ‡å®šç¼–ç 
        }

        # æ‰‹åŠ¨å°†payloadç¼–ç ä¸ºUTF-8 JSONå­—ç¬¦ä¸²
        json_payload = json.dumps(payload, ensure_ascii=False).encode('utf-8')

        response = requests.post(
            SILICONFLOW_API_URL,
            data=json_payload, # é€šè¿‡dataå‚æ•°å‘é€ç¼–ç åçš„JSON
            headers=headers,
            timeout=60  # å»¶é•¿è¶…æ—¶æ—¶é—´
        )

        response.raise_for_status()
        result = response.json()
        
        # æå–å›ç­”å†…å®¹å’Œæ€ç»´é“¾
        if "choices" in result and len(result["choices"]) > 0:
            message = result["choices"][0]["message"]
            content = message.get("content", "")
            reasoning = message.get("reasoning_content", "")
            
            # å¦‚æœæœ‰æ€ç»´é“¾ï¼Œåˆ™æ·»åŠ ç‰¹æ®Šæ ‡è®°ï¼Œä»¥ä¾¿å‰ç«¯å¤„ç†
            if reasoning:
                # æ·»åŠ æ€ç»´é“¾æ ‡è®°
                full_response = f"{content}<think>{reasoning}</think>"
                return full_response
            else:
                return content
        else:
            return "APIè¿”å›ç»“æœæ ¼å¼å¼‚å¸¸ï¼Œè¯·æ£€æŸ¥"
            
    except requests.exceptions.RequestException as e:
        logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‡ºé”™: {str(e)}")
        return f"è°ƒç”¨APIæ—¶å‡ºé”™: {str(e)}"
    except json.JSONDecodeError:
        logging.error("SiliconFlow APIè¿”å›éJSONå“åº”")
        return "APIå“åº”è§£æå¤±è´¥"
    except Exception as e:
        logging.error(f"è°ƒç”¨SiliconFlow APIæ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}")
        return f"å‘ç”ŸæœªçŸ¥é”™è¯¯: {str(e)}"

def hybrid_merge(semantic_results, bm25_results, alpha=0.7):
    """
    åˆå¹¶è¯­ä¹‰æœç´¢å’ŒBM25æœç´¢ç»“æœ
    
    å‚æ•°:
        semantic_results: å‘é‡æ£€ç´¢ç»“æœ (å­—å…¸æ ¼å¼ï¼ŒåŒ…å«ids, documents, metadatas)
        bm25_results: BM25æ£€ç´¢ç»“æœ (å­—å…¸åˆ—è¡¨ï¼ŒåŒ…å«id, score, content)
        alpha: è¯­ä¹‰æœç´¢æƒé‡ (0-1)
        
    è¿”å›:
        åˆå¹¶åçš„ç»“æœåˆ—è¡¨ [(doc_id, {'score': score, 'content': content, 'metadata': metadata}), ...]
    """
    merged_dict = {}
    global faiss_metadatas_map # Ensure we can access the global map
    
    # å¤„ç†è¯­ä¹‰æœç´¢ç»“æœ
    if (semantic_results and 
        isinstance(semantic_results.get('documents'), list) and len(semantic_results['documents']) > 0 and
        isinstance(semantic_results.get('metadatas'), list) and len(semantic_results['metadatas']) > 0 and
        isinstance(semantic_results.get('ids'), list) and len(semantic_results['ids']) > 0 and
        isinstance(semantic_results['documents'][0], list) and 
        isinstance(semantic_results['metadatas'][0], list) and 
        isinstance(semantic_results['ids'][0], list) and
        len(semantic_results['documents'][0]) == len(semantic_results['metadatas'][0]) == len(semantic_results['ids'][0])):
        
        num_results = len(semantic_results['documents'][0])
        # Assuming semantic_results are already ordered by relevance (higher is better)
        # A simple rank-based score, can be replaced if actual scores/distances are available and preferred
        for i, (doc_id, doc, meta) in enumerate(zip(semantic_results['ids'][0], semantic_results['documents'][0], semantic_results['metadatas'][0])):
            score = 1.0 - (i / max(1, num_results)) # Higher rank (smaller i) gets higher score
            merged_dict[doc_id] = {
                'score': alpha * score, 
                'content': doc,
                'metadata': meta
            }
    else:
        logging.warning("Semantic results are missing, have an unexpected format, or are empty. Skipping semantic part in hybrid merge.")
    
    # å¤„ç†BM25ç»“æœ
    if not bm25_results:
        return sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)
        
    valid_bm25_scores = [r['score'] for r in bm25_results if isinstance(r, dict) and 'score' in r]
    max_bm25_score = max(valid_bm25_scores) if valid_bm25_scores else 1.0
    
    for result in bm25_results:
        if not (isinstance(result, dict) and 'id' in result and 'score' in result and 'content' in result):
            logging.warning(f"Skipping invalid BM25 result item: {result}")
            continue
            
        doc_id = result['id']
        # Normalize BM25 score
        normalized_score = result['score'] / max_bm25_score if max_bm25_score > 0 else 0
        
        if doc_id in merged_dict:
            merged_dict[doc_id]['score'] += (1 - alpha) * normalized_score
        else:
            metadata = faiss_metadatas_map.get(doc_id, {}) # Get metadata from our global map
            merged_dict[doc_id] = {
                'score': (1 - alpha) * normalized_score,
                'content': result['content'],
                'metadata': metadata
            }
    
    merged_results = sorted(merged_dict.items(), key=lambda x: x[1]['score'], reverse=True)
    return merged_results

# æ–°å¢ï¼šæ›´æ–°æœ¬åœ°æ–‡æ¡£çš„BM25ç´¢å¼•
def update_bm25_index():
    """æ›´æ–°BM25ç´¢å¼•ï¼Œä»å†…å­˜ä¸­çš„æ˜ å°„åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
    global faiss_contents_map, faiss_id_order_for_index
    try:
        # Use the ordered list of IDs to ensure consistency
        doc_ids = faiss_id_order_for_index
        if not doc_ids:
            logging.warning("æ²¡æœ‰å¯ç´¢å¼•çš„æ–‡æ¡£ (FAISS IDåˆ—è¡¨ä¸ºç©º)")
            BM25_MANAGER.clear()
            return False

        # Retrieve documents in the correct order
        documents = [faiss_contents_map.get(doc_id, "") for doc_id in doc_ids]
        
        # Filter out any potential empty documents if necessary, though map access should be safe
        valid_docs_with_ids = [(doc_id, doc) for doc_id, doc in zip(doc_ids, documents) if doc]
        if not valid_docs_with_ids:
            logging.warning("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£å†…å®¹å¯ç”¨äºBM25ç´¢å¼•")
            BM25_MANAGER.clear()
            return False
            
        # Separate IDs and documents again for building the index
        final_doc_ids = [item[0] for item in valid_docs_with_ids]
        final_documents = [item[1] for item in valid_docs_with_ids]
            
        BM25_MANAGER.build_index(final_documents, final_doc_ids)
        logging.info(f"BM25ç´¢å¼•æ›´æ–°å®Œæˆï¼Œå…±ç´¢å¼• {len(final_doc_ids)} ä¸ªæ–‡æ¡£")
        return True
    except Exception as e:
        logging.error(f"æ›´æ–°BM25ç´¢å¼•å¤±è´¥: {str(e)}")
        return False

# æ–°å¢å‡½æ•°ï¼šè·å–ç³»ç»Ÿä½¿ç”¨çš„æ¨¡å‹ä¿¡æ¯
def get_system_models_info():
    """è¿”å›ç³»ç»Ÿä½¿ç”¨çš„å„ç§æ¨¡å‹ä¿¡æ¯"""
    models_info = {
        "åµŒå…¥æ¨¡å‹": "all-MiniLM-L6-v2",
        "åˆ†å—æ–¹æ³•": "RecursiveCharacterTextSplitter (chunk_size=800, overlap=150)",
        "æ£€ç´¢æ–¹æ³•": "å‘é‡æ£€ç´¢ + BM25æ··åˆæ£€ç´¢ (Î±=0.7)",
        "é‡æ’åºæ¨¡å‹": "äº¤å‰ç¼–ç å™¨ (sentence-transformers/distiluse-base-multilingual-cased-v2)",
        "ç”Ÿæˆæ¨¡å‹": "deepseek-r1 (7B/1.5B)",
        "åˆ†è¯å·¥å…·": "jieba (ä¸­æ–‡åˆ†è¯)"
    }
    return models_info

# æ–°å¢å‡½æ•°ï¼šè·å–æ–‡æ¡£åˆ†å—å¯è§†åŒ–æ•°æ®
def get_document_chunks(progress=gr.Progress()):
    """è·å–æ–‡æ¡£åˆ†å—ç»“æœç”¨äºå¯è§†åŒ–"""
    global faiss_contents_map, faiss_metadatas_map, faiss_id_order_for_index
    global chunk_data_cache # Ensure we can update the global cache
    try:
        progress(0.1, desc="æ­£åœ¨ä»å†…å­˜åŠ è½½æ•°æ®...")
        
        if not faiss_id_order_for_index:
            chunk_data_cache = []
            return [], "çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£ï¼Œè¯·å…ˆä¸Šä¼ å¹¶å¤„ç†æ–‡æ¡£ã€‚"
            
        progress(0.5, desc="æ­£åœ¨ç»„ç»‡åˆ†å—æ•°æ®...")
        
        doc_groups = {}
        # Iterate using the ordered IDs to reflect the FAISS index order conceptually
        for doc_id in faiss_id_order_for_index:
            doc = faiss_contents_map.get(doc_id, "")
            meta = faiss_metadatas_map.get(doc_id, {})
            if not doc: # Skip if content is somehow empty
                continue

            source = meta.get('source', 'æœªçŸ¥æ¥æº')
            if source not in doc_groups:
                doc_groups[source] = []
            
            doc_id_meta = meta.get('doc_id', 'æœªçŸ¥ID') # Get the original document ID from meta
            chunk_info = {
                "original_id": doc_id, # Keep the chunk-specific ID
                "doc_id": doc_id_meta,
                "content": doc[:200] + "..." if len(doc) > 200 else doc,
                "full_content": doc,
                "token_count": len(list(jieba.cut(doc))),
                "char_count": len(doc)
            }
            doc_groups[source].append(chunk_info)
        
        result_dicts = []
        result_lists = []
        
        # Keep track of chunks per source for indexing
        source_chunk_counters = {source: 0 for source in doc_groups.keys()}
        total_chunks = 0
        
        for source, chunks in doc_groups.items():
            num_chunks_in_source = len(chunks)
            for chunk in chunks:
                source_chunk_counters[source] += 1
                total_chunks += 1
                result_dict = {
                    "æ¥æº": source,
                    "åºå·": f"{source_chunk_counters[source]}/{num_chunks_in_source}",
                    "å­—ç¬¦æ•°": chunk["char_count"],
                    "åˆ†è¯æ•°": chunk["token_count"],
                    "å†…å®¹é¢„è§ˆ": chunk["content"],
                    "å®Œæ•´å†…å®¹": chunk["full_content"],
                    "åŸå§‹åˆ†å—ID": chunk["original_id"] # Add original ID for potential debugging
                }
                result_dicts.append(result_dict)
                
                result_lists.append([
                    source,
                    f"{source_chunk_counters[source]}/{num_chunks_in_source}",
                    chunk["char_count"],
                    chunk["token_count"],
                    chunk["content"]
                ])
        
        progress(1.0, desc="æ•°æ®åŠ è½½å®Œæˆ!")
        
        chunk_data_cache = result_dicts # Update the global cache
        summary = f"æ€»è®¡ {total_chunks} ä¸ªæ–‡æœ¬å—ï¼Œæ¥è‡ª {len(doc_groups)} ä¸ªä¸åŒæ¥æºã€‚"
        
        return result_lists, summary
    except Exception as e:
        chunk_data_cache = []
        return [], f"è·å–åˆ†å—æ•°æ®å¤±è´¥: {str(e)}"

# æ·»åŠ å…¨å±€ç¼“å­˜å˜é‡
chunk_data_cache = []

# æ–°å¢å‡½æ•°ï¼šæ˜¾ç¤ºåˆ†å—è¯¦æƒ…
def show_chunk_details(evt: gr.SelectData, chunks):
    """æ˜¾ç¤ºé€‰ä¸­åˆ†å—çš„è¯¦ç»†å†…å®¹"""
    try:
        if evt.index[0] < len(chunk_data_cache):
            selected_chunk = chunk_data_cache[evt.index[0]]
            return selected_chunk.get("å®Œæ•´å†…å®¹", "å†…å®¹åŠ è½½å¤±è´¥")
        return "æœªæ‰¾åˆ°é€‰ä¸­çš„åˆ†å—"
    except Exception as e:
        return f"åŠ è½½åˆ†å—è¯¦æƒ…å¤±è´¥: {str(e)}"

# ä¿®æ”¹å¸ƒå±€éƒ¨åˆ†ï¼Œæ·»åŠ ä¸€ä¸ªæ–°çš„æ ‡ç­¾é¡µ
with gr.Blocks(
    title="æœ¬åœ°RAGé—®ç­”ç³»ç»Ÿ",
    css="""
    /* å…¨å±€ä¸»é¢˜å˜é‡ */
    :root[data-theme="light"] {
        --text-color: #2c3e50;
        --bg-color: #ffffff;
        --panel-bg: #f8f9fa;
        --border-color: #e9ecef;
        --success-color: #4CAF50;
        --error-color: #f44336;
        --primary-color: #2196F3;
        --secondary-bg: #ffffff;
        --hover-color: #e9ecef;
        --chat-user-bg: #e3f2fd;
        --chat-assistant-bg: #f5f5f5;
    }

    :root[data-theme="dark"] {
        --text-color: #e0e0e0;
        --bg-color: #1a1a1a;
        --panel-bg: #2d2d2d;
        --border-color: #404040;
        --success-color: #81c784;
        --error-color: #e57373;
        --primary-color: #64b5f6;
        --secondary-bg: #2d2d2d;
        --hover-color: #404040;
        --chat-user-bg: #1e3a5f;
        --chat-assistant-bg: #2d2d2d;
    }

    /* å…¨å±€æ ·å¼ */
    body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        margin: 0;
        padding: 0;
        overflow-x: hidden;
        width: 100vw;
        height: 100vh;
    }

    .gradio-container {
        max-width: 100% !important;
        width: 100% !important;
        margin: 0 !important;
        padding: 0 1% !important;
        color: var(--text-color);
        background-color: var(--bg-color);
        min-height: 100vh;
    }
    
    /* ç¡®ä¿æ ‡ç­¾å†…å®¹æ’‘æ»¡ */
    .tabs.svelte-710i53 {
        margin: 0 !important;
        padding: 0 !important;
        width: 100% !important;
    }

    /* ä¸»é¢˜åˆ‡æ¢æŒ‰é’® */
    .theme-toggle {
        position: fixed;
        top: 20px;
        right: 20px;
        z-index: 1000;
        padding: 8px 16px;
        border-radius: 20px;
        border: 1px solid var(--border-color);
        background: var(--panel-bg);
        color: var(--text-color);
        cursor: pointer;
        transition: all 0.3s ease;
        font-size: 14px;
        display: flex;
        align-items: center;
        gap: 8px;
    }

    .theme-toggle:hover {
        background: var(--hover-color);
    }

    /* é¢æ¿æ ·å¼ */
    .left-panel {
        padding-right: 20px;
        border-right: 1px solid var(--border-color);
        background: var(--bg-color);
        width: 100%;
    }

    .right-panel {
        height: 100vh;
        background: var(--bg-color);
        width: 100%;
    }

    /* æ–‡ä»¶åˆ—è¡¨æ ·å¼ */
    .file-list {
        margin-top: 10px;
        padding: 12px;
        background: var(--panel-bg);
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* ç­”æ¡ˆæ¡†æ ·å¼ */
    .answer-box {
        min-height: 500px !important;
        background: var(--panel-bg);
        border-radius: 8px;
        padding: 16px;
        font-size: 15px;
        line-height: 1.6;
        border: 1px solid var(--border-color);
    }

    /* è¾“å…¥æ¡†æ ·å¼ */
    textarea {
        background: var(--panel-bg) !important;
        color: var(--text-color) !important;
        border: 1px solid var(--border-color) !important;
        border-radius: 8px !important;
        padding: 12px !important;
        font-size: 14px !important;
    }

    /* æŒ‰é’®æ ·å¼ */
    button.primary {
        background: var(--primary-color) !important;
        color: white !important;
        border-radius: 8px !important;
        padding: 8px 16px !important;
        font-weight: 500 !important;
        transition: all 0.3s ease !important;
    }

    button.primary:hover {
        opacity: 0.9;
        transform: translateY(-1px);
    }

    /* æ ‡é¢˜å’Œæ–‡æœ¬æ ·å¼ */
    h1, h2, h3 {
        color: var(--text-color) !important;
        font-weight: 600 !important;
    }

    .footer-note {
        color: var(--text-color);
        opacity: 0.8;
        font-size: 13px;
        margin-top: 12px;
    }

    /* åŠ è½½å’Œè¿›åº¦æ ·å¼ */
    #loading, .progress-text {
        color: var(--text-color);
    }

    /* èŠå¤©è®°å½•æ ·å¼ */
    .chat-container {
        border: 1px solid var(--border-color);
        border-radius: 8px;
        margin-bottom: 16px;
        max-height: 80vh;
        height: 80vh !important;
        overflow-y: auto;
        background: var(--bg-color);
    }

    .chat-message {
        padding: 12px 16px;
        margin: 8px;
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.5;
    }

    .chat-message.user {
        background: var(--chat-user-bg);
        margin-left: 32px;
        border-top-right-radius: 4px;
    }

    .chat-message.assistant {
        background: var(--chat-assistant-bg);
        margin-right: 32px;
        border-top-left-radius: 4px;
    }

    .chat-message .timestamp {
        font-size: 12px;
        color: var(--text-color);
        opacity: 0.7;
        margin-bottom: 4px;
    }

    .chat-message .content {
        white-space: pre-wrap;
    }

    /* æŒ‰é’®ç»„æ ·å¼ */
    .button-row {
        display: flex;
        gap: 8px;
        margin-top: 8px;
    }

    .clear-button {
        background: var(--error-color) !important;
    }

    /* APIé…ç½®æç¤ºæ ·å¼ */
    .api-info {
        margin-top: 10px;
        padding: 10px;
        border-radius: 5px;
        background: var(--panel-bg);
        border: 1px solid var(--border-color);
    }

    /* æ–°å¢: æ•°æ®å¯è§†åŒ–å¡ç‰‡æ ·å¼ */
    .model-card {
        background: var(--panel-bg);
        border-radius: 8px;
        padding: 16px;
        border: 1px solid var(--border-color);
        margin-bottom: 16px;
    }

    .model-card h3 {
        margin-top: 0;
        border-bottom: 1px solid var(--border-color);
        padding-bottom: 8px;
    }

    .model-item {
        display: flex;
        margin-bottom: 8px;
    }

    .model-item .label {
        flex: 1;
        font-weight: 500;
    }

    .model-item .value {
        flex: 2;
    }

    /* æ•°æ®è¡¨æ ¼æ ·å¼ */
    .chunk-table {
        border-radius: 8px;
        overflow: hidden;
        border: 1px solid var(--border-color);
    }

    .chunk-table th, .chunk-table td {
        border: 1px solid var(--border-color);
        padding: 8px;
    }

    .chunk-detail-box {
        min-height: 200px;
        padding: 16px;
        background: var(--panel-bg);
        border-radius: 8px;
        border: 1px solid var(--border-color);
        font-family: monospace;
        white-space: pre-wrap;
        overflow-y: auto;
    }
    """
) as demo:
    gr.Markdown("# ğŸ§  æ™ºèƒ½æ–‡æ¡£é—®ç­”ç³»ç»Ÿ")
    
    with gr.Tabs() as tabs:
        # ç¬¬ä¸€ä¸ªé€‰é¡¹å¡ï¼šé—®ç­”å¯¹è¯
        with gr.TabItem("ğŸ’¬ é—®ç­”å¯¹è¯"):
            with gr.Row(equal_height=True):
                # å·¦ä¾§æ“ä½œé¢æ¿ - è°ƒæ•´æ¯”ä¾‹ä¸ºåˆé€‚çš„å¤§å°
                with gr.Column(scale=5, elem_classes="left-panel"):
                    gr.Markdown("## ğŸ“‚ æ–‡æ¡£å¤„ç†åŒº")
                    with gr.Group():
                        file_input = gr.File(
                            label="ä¸Šä¼ PDFæ–‡æ¡£",
                            file_types=[".pdf"],
                            file_count="multiple"
                        )
                        upload_btn = gr.Button("ğŸš€ å¼€å§‹å¤„ç†", variant="primary")
                        upload_status = gr.Textbox(
                            label="å¤„ç†çŠ¶æ€",
                            interactive=False,
                            lines=2
                        )
                        file_list = gr.Textbox(
                            label="å·²å¤„ç†æ–‡ä»¶",
                            interactive=False,
                            lines=3,
                            elem_classes="file-list"
                        )
                    
                    # å°†é—®é¢˜è¾“å…¥åŒºç§»è‡³å·¦ä¾§é¢æ¿åº•éƒ¨
                    gr.Markdown("## â“ è¾“å…¥é—®é¢˜")
                    with gr.Group():
                        question_input = gr.Textbox(
                            label="è¾“å…¥é—®é¢˜",
                            lines=3,
                            placeholder="è¯·è¾“å…¥æ‚¨çš„é—®é¢˜...",
                            elem_id="question-input"
                        )
                        with gr.Row():
                            # æ·»åŠ è”ç½‘å¼€å…³
                            web_search_checkbox = gr.Checkbox(
                                label="å¯ç”¨è”ç½‘æœç´¢", 
                                value=False,
                                info="æ‰“å¼€åå°†åŒæ—¶æœç´¢ç½‘ç»œå†…å®¹ï¼ˆéœ€é…ç½®SERPAPI_KEYï¼‰"
                            )
                            
                            # æ·»åŠ æ¨¡å‹é€‰æ‹©ä¸‹æ‹‰æ¡†
                            model_choice = gr.Dropdown(
                                choices=["ollama", "siliconflow"],
                                value="ollama",
                                label="æ¨¡å‹é€‰æ‹©",
                                info="é€‰æ‹©ä½¿ç”¨æœ¬åœ°æ¨¡å‹æˆ–äº‘ç«¯æ¨¡å‹"
                            )
                            
                        with gr.Row():
                            ask_btn = gr.Button("ğŸ” å¼€å§‹æé—®", variant="primary", scale=2)
                            clear_btn = gr.Button("ğŸ—‘ï¸ æ¸…ç©ºå¯¹è¯", variant="secondary", elem_classes="clear-button", scale=1)
                    
                    # æ·»åŠ APIé…ç½®æç¤ºä¿¡æ¯
                    api_info = gr.HTML(
                        """
                        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
                            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
                            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
                            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
                        </div>
                        """
                    )

                # å³ä¾§å¯¹è¯åŒº - è°ƒæ•´æ¯”ä¾‹
                with gr.Column(scale=7, elem_classes="right-panel"):
                    gr.Markdown("## ğŸ“ å¯¹è¯è®°å½•")
                    
                    # å¯¹è¯è®°å½•æ˜¾ç¤ºåŒº
                    chatbot = gr.Chatbot(
                        label="å¯¹è¯å†å²",
                        height=600,  # å¢åŠ é«˜åº¦
                        elem_classes="chat-container",
                        show_label=False
                    )
                    
                    status_display = gr.HTML("", elem_id="status-display")
                    gr.Markdown("""
                    <div class="footer-note">
                        *å›ç­”ç”Ÿæˆå¯èƒ½éœ€è¦1-2åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…<br>
                        *æ”¯æŒå¤šè½®å¯¹è¯ï¼Œå¯åŸºäºå‰æ–‡ç»§ç»­æé—®
                    </div>
                    """)
        
        # ç¬¬äºŒä¸ªé€‰é¡¹å¡ï¼šåˆ†å—å¯è§†åŒ–
        with gr.TabItem("ğŸ“Š åˆ†å—å¯è§†åŒ–"):
            with gr.Row():
                with gr.Column(scale=1):
                    gr.Markdown("## ğŸ’¡ ç³»ç»Ÿæ¨¡å‹ä¿¡æ¯")
                    
                    # æ˜¾ç¤ºç³»ç»Ÿæ¨¡å‹ä¿¡æ¯å¡ç‰‡
                    models_info = get_system_models_info()
                    with gr.Group(elem_classes="model-card"):
                        gr.Markdown("### æ ¸å¿ƒæ¨¡å‹ä¸æŠ€æœ¯")
                        
                        for key, value in models_info.items():
                            with gr.Row():
                                gr.Markdown(f"**{key}**:", elem_classes="label")
                                gr.Markdown(f"{value}", elem_classes="value")
                
                with gr.Column(scale=2):
                    gr.Markdown("## ğŸ“„ æ–‡æ¡£åˆ†å—ç»Ÿè®¡")
                    refresh_chunks_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ†å—æ•°æ®", variant="primary")
                    chunks_status = gr.Markdown("ç‚¹å‡»æŒ‰é’®æŸ¥çœ‹åˆ†å—ç»Ÿè®¡")
            
            # åˆ†å—æ•°æ®è¡¨æ ¼å’Œè¯¦æƒ…
            with gr.Row():
                chunks_data = gr.Dataframe(
                    headers=["æ¥æº", "åºå·", "å­—ç¬¦æ•°", "åˆ†è¯æ•°", "å†…å®¹é¢„è§ˆ"],
                    elem_classes="chunk-table",
                    interactive=False,
                    wrap=True,
                    row_count=(10, "dynamic")
                )
            
            with gr.Row():
                chunk_detail_text = gr.Textbox(
                    label="åˆ†å—è¯¦æƒ…",
                    placeholder="ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡ŒæŸ¥çœ‹å®Œæ•´å†…å®¹...",
                    lines=8,
                    elem_classes="chunk-detail-box"
                )
                
            gr.Markdown("""
            <div class="footer-note">
                * ç‚¹å‡»è¡¨æ ¼ä¸­çš„è¡Œå¯æŸ¥çœ‹è¯¥åˆ†å—çš„å®Œæ•´å†…å®¹<br>
                * åˆ†è¯æ•°è¡¨ç¤ºä½¿ç”¨jiebaåˆ†è¯åçš„tokenæ•°é‡
            </div>
            """)


    # è¿›åº¦æ˜¾ç¤ºç»„ä»¶è°ƒæ•´åˆ°å·¦ä¾§é¢æ¿ä¸‹æ–¹
    with gr.Row(visible=False) as progress_row:
        gr.HTML("""
        <div class="progress-text">
            <span>å½“å‰è¿›åº¦ï¼š</span>
            <span id="current-step" style="color: #2b6de3;">åˆå§‹åŒ–...</span>
            <span id="progress-percent" style="margin-left:15px;color: #e32b2b;">0%</span>
        </div>
        """)

    # å®šä¹‰å‡½æ•°å¤„ç†äº‹ä»¶
    def clear_chat_history():
        return None, "å¯¹è¯å·²æ¸…ç©º"

    def process_chat(question, history, enable_web_search, model_choice):
        if history is None:
            history = []
        
        # æ›´æ–°æ¨¡å‹é€‰æ‹©ä¿¡æ¯çš„æ˜¾ç¤º
        api_text = """
        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
        </div>
        """ % (
            "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨", 
            "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
            "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
        )
        
        # å¦‚æœé—®é¢˜ä¸ºç©ºï¼Œä¸å¤„ç†
        if not question or question.strip() == "":
            history.append(("", "é—®é¢˜ä¸èƒ½ä¸ºç©ºï¼Œè¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜ã€‚"))
            return history, "", api_text
        
        # æ·»åŠ ç”¨æˆ·é—®é¢˜åˆ°å†å²
        history.append((question, ""))
        
        # åˆ›å»ºç”Ÿæˆå™¨
        resp_generator = stream_answer(question, enable_web_search, model_choice)
        
        # æµå¼æ›´æ–°å›ç­”
        for response, status in resp_generator:
            history[-1] = (question, response)
            yield history, "", api_text

    def update_api_info(enable_web_search, model_choice):
        api_text = """
        <div class="api-info" style="margin-top:10px;padding:10px;border-radius:5px;background:var(--panel-bg);border:1px solid var(--border-color);">
            <p>ğŸ“¢ <strong>åŠŸèƒ½è¯´æ˜ï¼š</strong></p>
            <p>1. <strong>è”ç½‘æœç´¢</strong>ï¼š%s</p>
            <p>2. <strong>æ¨¡å‹é€‰æ‹©</strong>ï¼šå½“å‰ä½¿ç”¨ <strong>%s</strong> %s</p>
        </div>
        """ % (
            "å·²å¯ç”¨" if enable_web_search else "æœªå¯ç”¨", 
            "Cloud DeepSeek-R1 æ¨¡å‹" if model_choice == "siliconflow" else "æœ¬åœ° Ollama æ¨¡å‹",
            "(éœ€è¦åœ¨.envæ–‡ä»¶ä¸­é…ç½®SERPAPI_KEY)" if enable_web_search else ""
        )
        return api_text

    # ç»‘å®šUIäº‹ä»¶
    upload_btn.click(
        process_multiple_pdfs,
        inputs=[file_input],
        outputs=[upload_status, file_list],
        show_progress=True
    )

    # ç»‘å®šæé—®æŒ‰é’®
    ask_btn.click(
        process_chat,
        inputs=[question_input, chatbot, web_search_checkbox, model_choice],
        outputs=[chatbot, question_input, api_info]
    )

    # ç»‘å®šæ¸…ç©ºæŒ‰é’®
    clear_btn.click(
        clear_chat_history,
        inputs=[],
        outputs=[chatbot, status_display]
    )

    # å½“åˆ‡æ¢è”ç½‘æœç´¢æˆ–æ¨¡å‹é€‰æ‹©æ—¶æ›´æ–°APIä¿¡æ¯
    web_search_checkbox.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )
    
    model_choice.change(
        update_api_info,
        inputs=[web_search_checkbox, model_choice],
        outputs=[api_info]
    )
    
    # æ–°å¢ï¼šåˆ†å—å¯è§†åŒ–åˆ·æ–°æŒ‰é’®äº‹ä»¶
    refresh_chunks_btn.click(
        fn=get_document_chunks,
        outputs=[chunks_data, chunks_status]
    )
    
    # æ–°å¢ï¼šåˆ†å—è¡¨æ ¼ç‚¹å‡»äº‹ä»¶
    chunks_data.select(
        fn=show_chunk_details,
        inputs=chunks_data,
        outputs=chunk_detail_text
    )

# ä¿®æ”¹JavaScriptæ³¨å…¥éƒ¨åˆ†
demo._js = """
function gradioApp() {
    // è®¾ç½®é»˜è®¤ä¸»é¢˜ä¸ºæš—è‰²
    document.documentElement.setAttribute('data-theme', 'dark');
    
    const observer = new MutationObserver((mutations) => {
        document.getElementById("loading").style.display = "none";
        const progress = document.querySelector('.progress-text');
        if (progress) {
            const percent = document.querySelector('.progress > div')?.innerText || '';
            const step = document.querySelector('.progress-description')?.innerText || '';
            document.getElementById('current-step').innerText = step;
            document.getElementById('progress-percent').innerText = percent;
        }
    });
    observer.observe(document.body, {childList: true, subtree: true});
}

function toggleTheme() {
    const root = document.documentElement;
    const currentTheme = root.getAttribute('data-theme');
    const newTheme = currentTheme === 'light' ? 'dark' : 'light';
    root.setAttribute('data-theme', newTheme);
}

// åˆå§‹åŒ–ä¸»é¢˜
document.addEventListener('DOMContentLoaded', () => {
    document.documentElement.setAttribute('data-theme', 'dark');
});
"""

# ä¿®æ”¹ç«¯å£æ£€æŸ¥å‡½æ•°
def is_port_available(port):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        s.settimeout(1)
        return s.connect_ex(('127.0.0.1', port)) != 0  # æ›´å¯é çš„æ£€æµ‹æ–¹å¼

def check_environment():
    """ç¯å¢ƒä¾èµ–æ£€æŸ¥"""
    try:
        # æ·»åŠ æ¨¡å‹å­˜åœ¨æ€§æ£€æŸ¥
        model_check = session.post(
            "http://localhost:11434/api/show",
            json={"name": "deepseek-r1:7b"},
            timeout=10
        )
        if model_check.status_code != 200:
            print("æ¨¡å‹æœªåŠ è½½ï¼è¯·å…ˆæ‰§è¡Œï¼š")
            print("ollama pull deepseek-r1:7b")
            return False
            
        # åŸæœ‰æ£€æŸ¥ä¿æŒä¸å˜...
        response = session.get(
            "http://localhost:11434/api/tags",
            proxies={"http": None, "https": None},  # ç¦ç”¨ä»£ç†
            timeout=5
        )
        if response.status_code != 200:
            print("OllamaæœåŠ¡å¼‚å¸¸ï¼Œè¿”å›çŠ¶æ€ç :", response.status_code)
            return False
        return True
    except Exception as e:
        print("Ollamaè¿æ¥å¤±è´¥:", str(e))
        return False

# æ–¹æ¡ˆ2ï¼šç¦ç”¨æµè§ˆå™¨ç¼“å­˜ï¼ˆæ·»åŠ metaæ ‡ç­¾ï¼‰
gr.HTML("""
<meta http-equiv="Cache-Control" content="no-cache, no-store, must-revalidate">
<meta http-equiv="Pragma" content="no-cache">
<meta http-equiv="Expires" content="0">
""")

# æ¢å¤ä¸»ç¨‹åºå¯åŠ¨éƒ¨åˆ†
if __name__ == "__main__":
    if not check_environment():
        exit(1)
    ports = [17995, 17996, 17997, 17998, 17999]
    selected_port = next((p for p in ports if is_port_available(p)), None)
    
    if not selected_port:
        print("æ‰€æœ‰ç«¯å£éƒ½è¢«å ç”¨ï¼Œè¯·æ‰‹åŠ¨é‡Šæ”¾ç«¯å£")
        exit(1)
        
    try:
        ollama_check = session.get("http://localhost:11434", timeout=5)
        if ollama_check.status_code != 200:
            print("OllamaæœåŠ¡æœªæ­£å¸¸å¯åŠ¨ï¼")
            print("è¯·å…ˆæ‰§è¡Œï¼šollama serve å¯åŠ¨æœåŠ¡")
            exit(1)
            
        webbrowser.open(f"http://127.0.0.1:{selected_port}")
        demo.launch(
            server_port=selected_port,
            server_name="0.0.0.0",
            show_error=True,
            ssl_verify=False,
            height=900
        )
    except Exception as e:
        print(f"å¯åŠ¨å¤±è´¥: {str(e)}")

