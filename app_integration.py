"""
å°†æŸ¥è¯¢å¤„ç†åŠŸèƒ½é›†æˆåˆ°Gradioç•Œé¢
"""

import gradio as gr
from query_processing import QueryProcessor, IntentType
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("app-integration")

def integrate_query_processing(original_stream_answer):
    """åŒ…è£…åŸå§‹stream_answerå‡½æ•°ï¼Œæ·»åŠ æŸ¥è¯¢å¤„ç†åŠŸèƒ½
    
    Args:
        original_stream_answer: åŸå§‹çš„stream_answerå‡½æ•°
        
    Returns:
        å¢å¼ºç‰ˆçš„stream_answerå‡½æ•°
    """
    processor = QueryProcessor()
    
    def enhanced_stream_answer(question, enable_web_search=False, model_choice="ollama", progress=None):
        """å¢å¼ºç‰ˆçš„stream_answerå‡½æ•°
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            enable_web_search: æ˜¯å¦å¯ç”¨ç½‘ç»œæœç´¢
            model_choice: æ¨¡å‹é€‰æ‹©
            progress: è¿›åº¦å›è°ƒå‡½æ•°
            
        Yields:
            ç”Ÿæˆçš„å›ç­”å’ŒçŠ¶æ€
        """
        if progress:
            progress(0.1, desc="åˆ†ææŸ¥è¯¢æ„å›¾...")
        
        # å¤„ç†æŸ¥è¯¢
        processed = processor.process_query(question)
        intent_type = processed['intent']
        needs_retrieval = processed['needs_retrieval']
        rewritten_queries = processed['rewritten_queries']
        
        # æä¾›å…³äºæŸ¥è¯¢å¤„ç†çš„åé¦ˆ
        intent_feedback = f"âœ“ æŸ¥è¯¢æ„å›¾: {intent_type}"
        if needs_retrieval:
            intent_feedback += f"\nâœ“ æŸ¥è¯¢æ”¹å†™: å·²ç”Ÿæˆ{len(rewritten_queries)-1}ä¸ªå˜ä½“"
        else:
            intent_feedback += "\nâœ“ ç›´æ¥å›ç­”: æ— éœ€æ£€ç´¢"
        
        yield intent_feedback, "åˆ†ææŸ¥è¯¢ä¸­..."
        
        if progress:
            progress(0.2, desc="å‡†å¤‡æ£€ç´¢...")
        
        # å¯¹äºå®ä½“ç±»æŸ¥è¯¢ï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹å›ç­”
        if not needs_retrieval:
            prompt = f"""ç”¨æˆ·æå‡ºäº†ä¸€ä¸ª{intent_type}ç±»å‹çš„é—®é¢˜ï¼Œè¯·ç›´æ¥ç”¨ä½ çš„çŸ¥è¯†å›ç­”ï¼Œæ— éœ€å¼•ç”¨å¤–éƒ¨èµ„æ–™ã€‚
é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""
            
            import requests
            
            try:
                if progress:
                    progress(0.5, desc="ç”Ÿæˆç›´æ¥å›ç­”...")
                
                response = requests.post(
                    "http://localhost:11434/api/generate",
                    json={
                        "model": "deepseek-r1:7b" if model_choice == "ollama" else model_choice,
                        "prompt": prompt,
                        "stream": True
                    },
                    timeout=60,
                    stream=True
                )
                
                answer = intent_feedback + "\n\n"
                
                for line in response.iter_lines():
                    if line:
                        import json
                        chunk = json.loads(line.decode()).get("response", "")
                        answer += chunk
                        yield answer, "ç”Ÿæˆç›´æ¥å›ç­”ä¸­..."
                
                final_answer = answer + "\n\n(æ­¤å›ç­”ç”±æ¨¡å‹ç›´æ¥ç”Ÿæˆï¼Œæœªä½¿ç”¨æ£€ç´¢å¢å¼º)"
                yield final_answer, "å®Œæˆ!"
                
            except Exception as e:
                error_msg = f"ç”Ÿæˆç›´æ¥å›ç­”æ—¶å‡ºé”™: {str(e)}"
                logger.error(error_msg)
                yield intent_feedback + "\n\n" + error_msg, "é‡åˆ°é”™è¯¯"
                
        else:
            # å¯¹äºéœ€è¦æ£€ç´¢çš„é—®é¢˜ï¼Œä½¿ç”¨åŸå§‹stream_answerè¿›è¡Œå¤„ç†
            # ä½†åœ¨æœ€ç»ˆå›ç­”ä¸­åŠ å…¥æ„å›¾åˆ†æä¿¡æ¯
            origin_generator = original_stream_answer(question, enable_web_search, model_choice, progress)
            
            first_response = True
            for response, status in origin_generator:
                if first_response:
                    # åœ¨ç¬¬ä¸€ä¸ªå“åº”å‰æ·»åŠ æ„å›¾åˆ†æä¿¡æ¯
                    enhanced_response = intent_feedback + "\n\n" + response
                    first_response = False
                else:
                    enhanced_response = intent_feedback + "\n\n" + response
                
                yield enhanced_response, status
    
    return enhanced_stream_answer

def integrate_into_UI(demo, original_stream_answer):
    """å°†æŸ¥è¯¢å¤„ç†åŠŸèƒ½é›†æˆåˆ°Gradioç•Œé¢
    
    Args:
        demo: Gradioåº”ç”¨å®ä¾‹
        original_stream_answer: åŸå§‹stream_answerå‡½æ•°
        
    Returns:
        ä¿®æ”¹åçš„Gradioåº”ç”¨å®ä¾‹
    """
    # åˆ›å»ºå¢å¼ºç‰ˆçš„stream_answerå‡½æ•°
    enhanced_stream_answer = integrate_query_processing(original_stream_answer)
    
    # æ·»åŠ æŸ¥è¯¢å¤„ç†è¯¦æƒ…æ˜¾ç¤ºåŒºåŸŸ
    with demo:
        with gr.Tabs():
            with gr.TabItem("ğŸ’¬ é—®ç­”å¯¹è¯"):
                with gr.Row():
                    with gr.Column(scale=5):
                        query_intent_display = gr.Textbox(
                            label="æŸ¥è¯¢æ„å›¾åˆ†æ",
                            value="åœ¨æé—®åæ˜¾ç¤ºæŸ¥è¯¢æ„å›¾åˆ†æç»“æœ...",
                            lines=2,
                            interactive=False
                        )
    
    # æ·»åŠ ä¸€ä¸ªå‡½æ•°ç”¨äºæ›´æ–°UIï¼Œæ˜¾ç¤ºæŸ¥è¯¢å¤„ç†ç»“æœ
    def update_query_info(question):
        processor = QueryProcessor()
        result = processor.process_query(question)
        
        info = f"æŸ¥è¯¢ã€Œ{question}ã€çš„æ„å›¾ç±»å‹: {result['intent']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})\n"
        if result['needs_retrieval']:
            variant_count = len(result['rewritten_queries']) - 1  # å‡å»åŸå§‹æŸ¥è¯¢
            info += f"éœ€è¦æ£€ç´¢: æ˜¯ï¼Œå·²ç”Ÿæˆ{variant_count}ä¸ªæŸ¥è¯¢å˜ä½“"
        else:
            info += "éœ€è¦æ£€ç´¢: å¦ï¼Œå°†ç›´æ¥å›ç­”"
        
        return info
    
    # å°†åŸå§‹stream_answeræ›¿æ¢ä¸ºå¢å¼ºç‰ˆ
    import types
    if hasattr(demo, 'stream_answer'):
        demo.stream_answer = enhanced_stream_answer
    
    # ä¿®æ”¹process_chatå‡½æ•°ä»¥ä½¿ç”¨enhanced_stream_answer
    if hasattr(demo, 'process_chat'):
        original_process_chat = demo.process_chat
        
        def enhanced_process_chat(question, history, enable_web_search, model_choice):
            # æ›´æ–°æŸ¥è¯¢æ„å›¾åˆ†ææ˜¾ç¤º
            query_intent_display.update(update_query_info(question))
            
            # è°ƒç”¨åŸå§‹process_chatå‡½æ•°
            return original_process_chat(question, history, enable_web_search, model_choice)
        
        demo.process_chat = enhanced_process_chat
    
    return demo 